{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjBeTMLJQJuQ"
   },
   "source": [
    "# Object Detection with Region Based Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uYOhP5GR2L0"
   },
   "source": [
    "You should have seen in our previous posts that Convolutional Neural Network is the state of the art for any computer vision task like\n",
    "\n",
    "- [Image classification](https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/)\n",
    "\n",
    "\n",
    "- [Semantic Segmentation](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/)\n",
    "\n",
    "In this notebook we will look at another computer vision application called object detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNatz6V4Z3JZ"
   },
   "source": [
    "# Object Localization\n",
    "\n",
    "In addition to classify an image, we want to know where in the image the object is. So we want to classify the image as cat/dog/..etc and we want to draw a bounding box around the region of the object in the image.\n",
    "\n",
    " \n",
    " But Object localization is pretty simple, it classifies an image like image classiifcation and draw a bounding box around the classified image. So if more that 1 class is present in the image, it ignores the other class and classifies image as a single class and draw box around it.\n",
    " \n",
    " Now given an image, the model will return the probability of the content of the image belonging to different known classes and the model will also return 4 floating values of the coordinates of the bounding box.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SOkRRaEl40c"
   },
   "source": [
    "# Object Detection\n",
    "  \n",
    "  Object Detection is the next step of Object Localization where the task remains the same(classification and bounding box) but obejct detection input images may have multiple known class objects in a single image and the task is to draw a bounding box around all the objects and classify each object.\n",
    "  \n",
    "  The main challenge here is that there might be a varying number of objects in every input image.\n",
    "\n",
    "\n",
    "- ## Sliding Window Approach\n",
    "  Sliding window is one of the oldest approach in object detection where the input image is split into multiple crops and each crop of the image is classified and if the crop contains a class, then the crop is decided as the bounding box. But this approach is never used in practice as each input image may have 1000s of such crops and each crop passing through the network for classification may take time.\n",
    "  \n",
    "- ## Region Proposal (RCNN)\n",
    "  Image processing techniques are used to make list of proposed regions in the input image which are then sent through the network for classification. But this is computationally more efficient than sliding window approach as only fewer potential crops which may contain the object is classified by the network.\n",
    "  \n",
    "  ![](https://cdn-images-1.medium.com/max/800/1*REPHY47zAyzgbNKC6zlvBQ.png)\n",
    "  Image Source :  [Ross Girshick et al](https://arxiv.org/pdf/1311.2524.pdf)\n",
    "  \n",
    "  RCNN is better than sliding window, but its still computationally expensive as the network has to classify all the region proposals. It takes around 30-40s for inference of a single image.\n",
    "  \n",
    "  \n",
    "- ## Fast Region Proposal (Fast RCNN)\n",
    "  In fast RCNN, rather than getting region proposals and classifying each region proposals, the input image is sent into the CNN network which gives a feature map of the image. Again some region proposals are used but now we get the region proposals from the feature map of the image and these feature maps are classified. This reduces the computation as some of the CNN layers are common for the whole image. \n",
    "  \n",
    "  ![](https://cdn-images-1.medium.com/max/800/1*0pMP3aY8blSpva5tvWbnKA.png)\n",
    "  Image Source : [Ross Gishich](https://arxiv.org/pdf/1504.08083.pdf)\n",
    "  \n",
    "\n",
    "- ## Faster R-CNN\n",
    "  The idea of Faster R-CNN is to use CNNs to propose potential region of interest and the network is called Region Proposal Network. After getting the region proposals , its just like Fast RCNN, we use every regions for classification.\n",
    "![](https://www.researchgate.net/profile/Giang_Son_Tran/publication/324549019/figure/fig1/AS:649929152266241@1531966593689/Faster-R-CNN-Architecture-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3joSB5Ta7t5j"
   },
   "source": [
    "# Object Detection, Instance Segmentation and Person Keypoint Detection in PyTorch\n",
    "\n",
    "It is fine if you don't understand every detail of the models discussed above. If you want to learn more about all of these models and many more application and concepts of Deep Learning and Computer Vision indetail,  check out the official [Deep Learning and Computer Vision courses](https://opencv.org/courses/) by OpenCV.org. \n",
    "\n",
    "Now we will use pre trained models in PyTorch for\n",
    "  - Object Detection\n",
    "  - Instance Segmentation \n",
    "  - Person Keypoint Detection\n",
    "  \n",
    "  \n",
    "  All the pretrained models in pytorch can be found in [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNWsNDZQ8HF3"
   },
   "source": [
    "# Object Detection with PyTorch\n",
    "\n",
    "The pretrained Faster-RCNN ResNet-50 model we are going to use expects the input image tensor to be in the form ```[n, c, h, w]``` \n",
    "where \n",
    "- n is the number of images\n",
    "- c is the number of channels , for RGB images its 3\n",
    "- h is the height of the image\n",
    "- w is the widht of the image\n",
    "\n",
    "The model will return\n",
    "- Bounding boxes [x0, y0, x1, y1]  all all predicted classes of shape (N,4) where N is the number of classes predicted by the model to be present in the image.\n",
    "- Labels of all predicted classes.\n",
    "- Scores of each predicted label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWOd_NvOYY-P"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# get the pretrained model from torchvision.models\n",
    "# Note: pretrained=True will get the pretrained weights for the model.\n",
    "# model.eval() to use the model for inference\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Class labels from official PyTorch documentation for the pretrained model\n",
    "# Note that there are some N/A's \n",
    "# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "# we will use the same list for this notebook\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "\n",
    "def get_prediction(img_path, threshold):\n",
    "  \"\"\"\n",
    "  get_prediction\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - threshold - threshold value for prediction score\n",
    "    method:\n",
    "      - Image is obtained from the image path\n",
    "      - the image is converted to image tensor using PyTorch's Transforms\n",
    "      - image is passed through the model to get the predictions\n",
    "      - class, box coordinates are obtained, but only prediction score > threshold\n",
    "        are chosen.\n",
    "    \n",
    "  \"\"\"\n",
    "  img = Image.open(img_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  pred = model([img])\n",
    "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n",
    "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n",
    "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "  pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
    "  pred_boxes = pred_boxes[:pred_t+1]\n",
    "  pred_class = pred_class[:pred_t+1]\n",
    "  return pred_boxes, pred_class\n",
    "  \n",
    "\n",
    "\n",
    "def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
    "  \"\"\"\n",
    "  object_detection_api\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "      - threshold - threshold value for prediction score\n",
    "      - rect_th - thickness of bounding box\n",
    "      - text_size - size of the class label text\n",
    "      - text_th - thichness of the text\n",
    "    method:\n",
    "      - prediction is obtained from get_prediction method\n",
    "      - for each prediction, bounding box is drawn and text is written \n",
    "        with opencv\n",
    "      - the final image is displayed\n",
    "  \"\"\"\n",
    "  boxes, pred_cls = get_prediction(img_path, threshold)\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  for i in range(len(boxes)):\n",
    "    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
    "    cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "  plt.figure(figsize=(20,30))\n",
    "  plt.imshow(img)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2Mo500ZJi1i"
   },
   "source": [
    "Let's try one more complex example\n",
    "```\n",
    "!wget https://cdn.pixabay.com/photo/2013/07/05/01/08/traffic-143391_960_720.jpg -O traffic.jpg\n",
    "  \n",
    "object_detection_api('/content/traffic.jpg', rect_th=2, text_th=1, text_size=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "R8Q4sr3sYZD3",
    "outputId": "c370d6d4-e8e5-47b4-f406-7aa1f0b72fdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jay.prakash.thakur\\AppData\\Local\\Continuum\\anaconda3\\envs\\CVND\\lib\\site-packages\\torch\\nn\\functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    }
   ],
   "source": [
    "# download an image for inference\n",
    "# !wget https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg -O people.jpg\n",
    "\n",
    "# use the api pipeline for object detection\n",
    "# the threshold is set manually, the model sometimes predict \n",
    "# random structures as some object, so we set a threshold to filter\n",
    "# better prediction scores.\n",
    "object_detection_api('./people.jpg', threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "colab_type": "code",
    "id": "Qwlq1NJKS4dS",
    "outputId": "9d4fd60e-4626-4591-fbdc-80c439793fb8"
   },
   "outputs": [],
   "source": [
    "# !wget https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/10best-cars-group-cropped-1542126037.jpg -O cars.jpg\n",
    "  \n",
    "object_detection_api('./cars.jpg', rect_th=6, text_th=5, text_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "2W86i2oxWyH2",
    "outputId": "7037d4ab-424f-4fd2-f5c0-9c82fca415b7"
   },
   "outputs": [],
   "source": [
    "# !wget https://cdn.pixabay.com/photo/2013/07/05/01/08/traffic-143391_960_720.jpg -O traffic_scene.jpg\n",
    "  \n",
    "object_detection_api('./traffic_scene.jpg', rect_th=2, text_th=1, text_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "tApZDCJGVcxQ",
    "outputId": "84a60471-174b-4814-e632-e9fc0d12dcd8"
   },
   "outputs": [],
   "source": [
    "# !wget https://images.unsplash.com/photo-1458169495136-854e4c39548a -O traffic_scene2.jpg\n",
    "  \n",
    "object_detection_api('./traffic_scene2.jpg', rect_th=15, text_th=7, text_size=5, threshold=0.8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8st2TuBIQkSJ"
   },
   "source": [
    "# Comparing the inference time of model in CPU & GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OYNHMIih4up"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_inference_time(image_path, gpu=False):\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  model.eval()\n",
    "  img = Image.open(image_path)\n",
    "  transform = T.Compose([T.ToTensor()])\n",
    "  img = transform(img)\n",
    "  if gpu:\n",
    "    model.cuda()\n",
    "    img = img.cuda()\n",
    "  else:\n",
    "    model.cpu()\n",
    "    img = img.cpu()\n",
    "  start_time = time.time()\n",
    "  pred = model([img])\n",
    "  end_time = time.time()\n",
    "  return end_time-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxkL9LRPXa5X"
   },
   "source": [
    "## Inference time for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ptbJ57cUSfut",
    "outputId": "7849439d-f7a8-41f6-e2cc-ed3436261e86"
   },
   "outputs": [],
   "source": [
    "cpu_time = sum([check_inference_time('./girl_cars.jpg', gpu=False) for _ in range(10)])/10.0\n",
    "# gpu_time = sum([check_inference_time('./girl_cars.jpg', gpu=True) for _ in range(10)])/10.0\n",
    "\n",
    "print('\\nAverage Time take by the model with CPU = {}s'.format(cpu_time))\n",
    "\n",
    "# print('\\n\\nAverage Time take by the model with GPU = {}s\\nAverage Time take by the model with CPU = {}s'.format(gpu_time, cpu_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch_RCNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
